{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Search and Question-Answering System with Qdrant and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook demonstrates the following steps:\n",
    "1. Documents are read and converted into numerical embeddings.\n",
    "2. The embeddings are stored in a Qdrant vector database for efficient similarity searches.\n",
    "3. Queries are processed using a Language Model (LLM) and the embeddings to retrieve relevant answers.\n",
    "4. Results are refined through reranking, and a structured prompt template ensures clarity in the final response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1760\\2602444415.py:5: UserWarning: Failed to obtain server version. Unable to check client-server compatibility. Set check_version=False to skip version check.\n",
      "  client = qdrant_client.QdrantClient(\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qdrant: A vector database for storing and searching embeddings (numerical representations of text)\n",
    "* Connects to a Qdrant instance running locally on port 6333. The collection (chat_with_docs) in Qdrant are like tables in databases will store the document embeddings.\n",
    "* client = qdrant_client.QdrantClient(...) initializes a QdrantClient instance, connecting it to a Qdrant server running locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The loader is a SimpleDirectoryReader instance, read and extract text from files (e.g., PDFs) in a specified directory, recursively.\n",
    "* It loads document content into a format we can work with.\n",
    "* Reads all PDF files in the ./docs directory and loads their content into docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize a QdrantVectorStore object by passing the previously created Qdrant client and a name for the collection.\n",
    "* QdrantVectorStore: Stores the embeddings in Qdrant.\n",
    "* VectorStoreIndex: Manages how documents and their embeddings are organized and queried.\n",
    "* Creates an index for the documents (docs) by converting them into embeddings and saving these embeddings in Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embedding model and index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The HuggingFaceEmbedding class use Hugging Face models to generate embeddings for text data. In this case, we use pretrained model \"BAAI/bge-large-en-v1.5\" by the Beijing Academy of Artificial Intelligence (BAAI).\n",
    "* Next, configure embed_model as the default embedding model in Settings. It ensures that the same model is used throughout our RAG pipeline to maintain consistency in embedding generation.\n",
    "* Finally, invoke the create_index function we defined earlier, passing in docs (the list of loaded documents). As discussed above, this function converts each document into an embedding using embed_model and stores the embeddings in Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ollama: A language model that processes and generates responses to user queries.\n",
    "* Request_timeout of 120 seconds for requests to the LLM to ensure that the system doesn’t get stuck if the model takes too long to respond.\n",
    "* Set the above LLM instance as the default language model in Settings, making it the primary model used in this RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "            Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\n",
    "\"\"\"\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PromptTemplate: A predefined structure for how the LLM will process queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SentenceTransformerRerank: Refines query results by re-ranking them using a more accurate model cross-encoder.\n",
    "* Ensures only the top 3 most relevant results are shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The query engine integrates the retrieval, re-ranking, and prompt-based response generation steps\n",
    "* Uses the index to retrieve the top 10 similar documents based on the query.\n",
    "* Applies reranking to refine the results.\n",
    "* Generates a final answer using the LLM based on the query and document context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='DSPy stands for Demonstrating Self-Improving Pipelines, and it refers to a programming model that translates prompting techniques into parameterized declarative modules. This means that DSPy allows developers to define prompts (or instructions) in a specific way using natural language signatures, which can be compiled into efficient and effective LMs (Language Models).', source_nodes=[NodeWithScore(node=TextNode(id_='a24aa8d1-0a0c-477c-b5e2-e8089870e161', embedding=None, metadata={'page_label': '2', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1bd94683-dece-4cb8-ad19-fb4a44162439', node_type='4', metadata={'page_label': '2', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, hash='1269e7b49ca3fb54493c591deea9fab3e77d37782de89005419f768f5fcf297c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='01ebb07e-1449-4583-950e-150e5c150d7f', node_type='1', metadata={'page_label': '2', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, hash='e391a7195b4df54ffd950023b2978d465978d789fd49b2a6deeab1862f0bae9a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2018) with explorations of chain\\nof thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and\\nagent loops. Our evaluations use a number of different compiling strategies effectively and show\\nthat straightforward DSPy programs outperform systems using hand-crafted prompts, while also\\nallowing our programs to use much smaller and hence more efficient LMs effectively.\\nOverall, this work proposes the first programming model that translates prompting techniques into\\nparameterized declarative modules and introduces an effective compiler with general optimiza-\\ntion strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contri-\\nbutions are empirical and algorithmic: with DSPy, we have found that we can implement very\\nshort programs that can bootstrap self-improving multi-stage NLP systems using LMs as small as\\nllama2-13b-chat and T5-Large (770M parameters). Without hand-crafted prompts and within\\nminutes to tens of minutes of compiling, compositions of DSPy modules can raise the quality of\\nsimple programs from 33% to 82% (Sec 6) and from 32% to 46% (Sec 7) for GPT-3.5 and, simi-\\nlarly, from 9% to 47% (Sec 6) and from 22% to 41% (Sec 7) for llama2-13b-chat.\\n1DSPy is pronounced dee-ess-pie. It’s the second iteration of our earlier Demonstrate–Search–Predict\\nframework (DSP; Khattab et al. 2022). This paper introduces the key concepts in DSPy. For more extensive and\\nup-to-date documentation of the framework, we refer readers to https://github.com/stanfordnlp/dspy.\\n2We derive the name tele-prompters from the notion of abstracting and automating the task of prompting,\\nin particular, such that it happens at a distance, without manual intervention.\\n2', mimetype='text/plain', start_char_idx=3355, end_char_idx=5088, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(-1.7459303)), NodeWithScore(node=TextNode(id_='b27d57c6-21f2-4403-8964-a80a4aaae772', embedding=None, metadata={'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7e0465b9-d946-4c39-be1b-e803632a6327', node_type='4', metadata={'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, hash='18a4ee724d1f0e3fe69ecce5c5a25dfa79726966d5c4c799387d33a75e1ca812'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d79f142e-c155-4b1b-9831-5f82993f3127', node_type='1', metadata={}, hash='4406c6bccfa1452bab9257b4a1c28b31cb1ff6cda63c0127e6538ce9718cc1c8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Preprint\\n3.1 N ATURAL LANGUAGE SIGNATURES CAN ABSTRACT PROMPTING & FINETUNING\\nInstead of free-form string prompts, DSPy programs use natural language signatures to assign work\\nto the LM. A DSPy signature isnatural-language typed declaration of a function: a short declarative\\nspec that tells DSPy what a text transformation needs to do (e.g., “consume questions and return\\nanswers”), rather than how a specific LM should be prompted to implement that behavior. More\\nformally, a DSPy signature is a tuple of input fields and output fields (and an optional instruction).\\nA field consists offield name and optional metadata.4 In typical usage, the roles of fields are inferred\\nby DSPy as a function of field names. For instance, the DSPy compiler will use in-context learning\\nto interpret questiondifferently from answer and will iteratively refine its usage of these fields.\\nSignatures offer two benefits over prompts: they can be compiled into self-improving and pipeline-\\nadaptive prompts or finetunes. This is primarily done by bootstrapping (Sec 4) useful demonstrating\\nexamples for each signature. Additionally, they handle structured formatting and parsing logic to\\nreduce (or, ideally, avoid) brittle string manipulation in user programs.\\nIn practice, DSPy signatures can be expressed with a shorthand notation likequestion -> answer,\\nso that line 1 in the following is a complete DSPy program for a basic question-answering system\\n(with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):\\n1 qa = dspy.Predict(\"question -> answer\")\\n2 qa(question=\"Where is Guaran ´ı spoken?\")\\n3 # Out: Prediction(answer=’Guaran ´ı is spoken mainly in South America.’)\\nIn the shorthand notation, each field’s name indicates the semantic role that the input (or output)\\nfield plays in the transformation. DSPy will parse this notation and expand the field names into\\nmeaningful instructions for the LM, so that english document -> french translation would\\nprompt for English to French translation. When needed, DSPy offers more advanced programming\\ninterfaces for expressing more explicit constraints on signatures (Appendix A).\\n3.2 P ARAMETERIZED & TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\\nAkin to type signatures in programming languages, DSPy signatures simply define an interface and\\nprovide type-like hints on the expected behavior. To use a signature, we must declare amodule with\\nthat signature, like we instantiated a Predict module above. A module declaration like this returns\\na function having that signature.\\nThe Predict Module The core module for working with signatures in DSPy isPredict(simplified\\npseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to\\nuse (initially None, but otherwise overrides the default LM for this module), and a list of demon-\\nstrations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as\\na callable function: it takes in keyword arguments corresponding to the signature input fields (e.g.,\\nquestion), formats a prompt to implement the signature and includes the appropriate demonstra-\\ntions, calls the LM, and parses the output fields. When Predict detects it’s being used in compile\\nmode, it will also internally track input/output traces to assist the teleprompter at bootstrapping the\\ndemonstrations.\\nOther Built-in ModulesDSPy modules translate prompting techniques into modular functions that\\nsupport any signature, contrasting with the standard approach of prompting LMs with task-specific\\ndetails (e.g., hand-written few-shot examples). To this end, DSPy includes a number of more sophis-\\nticated modules like ChainOfThought, ProgramOfThought, MultiChainComparison, and ReAct.5\\nThese can all be used interchangeably to implement a DSPy signature. For instance, simply chang-\\n4String descriptions of the task and the fields are also optional and usually omitted. Fields can carry optional\\nfield prefix and description. By default, fields are assumed to hold free-form strings; we are actively exploring\\noptional data type as a way to specify constraints on valid values (e.g.,boolor int) and more gracefully handle\\nformatting and parsing logic, though this feature is not core to DSPy at the time of writing.\\n5These modules generalize prompting techniques from the literature, respectively, by Wei et al. (2022),\\nChen et al. (2022), Yoran et al. (2023), and Yao et al.', mimetype='text/plain', start_char_idx=0, end_char_idx=4418, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(-1.8540092)), NodeWithScore(node=TextNode(id_='d79f142e-c155-4b1b-9831-5f82993f3127', embedding=None, metadata={'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7e0465b9-d946-4c39-be1b-e803632a6327', node_type='4', metadata={'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, hash='18a4ee724d1f0e3fe69ecce5c5a25dfa79726966d5c4c799387d33a75e1ca812'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b27d57c6-21f2-4403-8964-a80a4aaae772', node_type='1', metadata={'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, hash='5ce5fba4ccb808af2ca584b64d58caa20b608862d0817a1127284565985a6250')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='To this end, DSPy includes a number of more sophis-\\nticated modules like ChainOfThought, ProgramOfThought, MultiChainComparison, and ReAct.5\\nThese can all be used interchangeably to implement a DSPy signature. For instance, simply chang-\\n4String descriptions of the task and the fields are also optional and usually omitted. Fields can carry optional\\nfield prefix and description. By default, fields are assumed to hold free-form strings; we are actively exploring\\noptional data type as a way to specify constraints on valid values (e.g.,boolor int) and more gracefully handle\\nformatting and parsing logic, though this feature is not core to DSPy at the time of writing.\\n5These modules generalize prompting techniques from the literature, respectively, by Wei et al. (2022),\\nChen et al. (2022), Yoran et al. (2023), and Yao et al. (2022) and, in doing so, generalize the ideas on zero-shot\\nprompting and rationale self-generation from Kojima et al. (2022), Zelikman et al. (2022), Zhang et al. (2022),\\nand Huang et al. (2022) to parameterized modules that can bootstrap arbitrary multi-stage pipelines.\\n4', mimetype='text/plain', start_char_idx=3588, end_char_idx=4692, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(-2.0504665))], metadata={'a24aa8d1-0a0c-477c-b5e2-e8089870e161': {'page_label': '2', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, 'b27d57c6-21f2-4403-8964-a80a4aaae772': {'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}, 'd79f142e-c155-4b1b-9831-5f82993f3127': {'page_label': '4', 'file_name': 'dspy.pdf', 'file_path': 'e:\\\\ML Practice\\\\DailyDose of DS\\\\basic_RAG_application\\\\rag_project\\\\docs\\\\dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2024-11-02', 'last_modified_date': '2024-11-02'}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy stands for Demonstrating Self-Improving Pipelines, and it refers to a programming model that translates prompting techniques into parameterized declarative modules. This means that DSPy allows developers to define prompts (or instructions) in a specific way using natural language signatures, which can be compiled into efficient and effective LMs (Language Models)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
